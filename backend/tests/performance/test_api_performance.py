#!/usr/bin/env python3
"""
T105: å¾Œç«¯ API æ•ˆèƒ½æ¸¬è©¦
ç›®æ¨™ï¼š<200ms å›æ‡‰æ™‚é–“

ä½¿ç”¨ pytest-benchmark é€²è¡Œæ•ˆèƒ½æ¸¬è©¦
"""

import pytest
import asyncio
import time
import statistics
from typing import List, Dict, Any
from unittest.mock import Mock, AsyncMock, patch
import httpx
import sys
import os

# æ·»åŠ  src åˆ°è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '../../src'))

from main import app
from fastapi.testclient import TestClient


class APIPerformanceTest:
    """API æ•ˆèƒ½æ¸¬è©¦é¡åˆ¥"""

    def __init__(self):
        self.client = TestClient(app)
        self.response_times: List[float] = []
        self.performance_target = 0.2  # 200ms target

    def time_request(self, method: str, url: str, **kwargs) -> Dict[str, Any]:
        """æ¸¬é‡å–®ä¸€è«‹æ±‚çš„å›æ‡‰æ™‚é–“"""
        start_time = time.time()

        if method.upper() == 'GET':
            response = self.client.get(url, **kwargs)
        elif method.upper() == 'POST':
            response = self.client.post(url, **kwargs)
        elif method.upper() == 'PUT':
            response = self.client.put(url, **kwargs)
        elif method.upper() == 'DELETE':
            response = self.client.delete(url, **kwargs)

        end_time = time.time()
        response_time = end_time - start_time

        return {
            'response_time': response_time,
            'status_code': response.status_code,
            'response': response
        }

    def run_performance_test(self, method: str, url: str, iterations: int = 100, **kwargs) -> Dict[str, Any]:
        """åŸ·è¡Œæ•ˆèƒ½æ¸¬è©¦ä¸¦æ”¶é›†çµ±è¨ˆè³‡æ–™"""
        response_times = []
        status_codes = []

        for i in range(iterations):
            result = self.time_request(method, url, **kwargs)
            response_times.append(result['response_time'])
            status_codes.append(result['status_code'])

        return {
            'avg_response_time': statistics.mean(response_times),
            'median_response_time': statistics.median(response_times),
            'min_response_time': min(response_times),
            'max_response_time': max(response_times),
            'p95_response_time': statistics.quantiles(response_times, n=20)[18],  # 95th percentile
            'p99_response_time': statistics.quantiles(response_times, n=100)[98],  # 99th percentile
            'std_deviation': statistics.stdev(response_times) if len(response_times) > 1 else 0,
            'success_rate': sum(1 for code in status_codes if 200 <= code < 300) / len(status_codes) * 100,
            'total_requests': iterations
        }


@pytest.fixture
def performance_tester():
    """æ•ˆèƒ½æ¸¬è©¦å™¨ fixture"""
    return APIPerformanceTest()


@pytest.fixture
def mock_dependencies():
    """æ¨¡æ“¬å¤–éƒ¨ä¾è³´ä»¥å°ˆæ³¨æ–¼ API æ•ˆèƒ½"""
    with patch('src.database.get_db') as mock_db, \
         patch('src.services.question_set_file_manager.QuestionSetFileManager') as mock_qsm, \
         patch('src.services.vm_cluster_service.VMClusterService') as mock_vcs:

        # è¨­å®šåŸºæœ¬çš„æ¨¡æ“¬å›æ‡‰
        mock_db_session = Mock()
        mock_db.return_value = mock_db_session

        mock_qsm_instance = Mock()
        mock_qsm.return_value = mock_qsm_instance

        mock_vcs_instance = Mock()
        mock_vcs.return_value = mock_vcs_instance

        yield {
            'db': mock_db_session,
            'qsm': mock_qsm_instance,
            'vcs': mock_vcs_instance
        }


class TestVMConfigAPIPerformance:
    """VM é…ç½® API æ•ˆèƒ½æ¸¬è©¦"""

    def test_get_vm_configs_performance(self, performance_tester, mock_dependencies):
        """æ¸¬è©¦ GET /api/v1/vm-configs æ•ˆèƒ½"""
        # æ¨¡æ“¬å›æ‡‰è³‡æ–™
        mock_configs = [
            {
                'id': f'config-{i}',
                'name': f'æ¸¬è©¦å¢é›† {i}',
                'description': f'æ¸¬è©¦ç”¨å¢é›† {i}',
                'nodes': [{'name': 'master-1', 'ip': f'192.168.1.{10+i}', 'roles': ['master']}],
                'ssh_user': 'ubuntu',
                'created_by': 'test_user'
            } for i in range(10)
        ]

        with patch('src.api.endpoints.vm_configs.get_vm_configs') as mock_get:
            mock_get.return_value = mock_configs

            stats = performance_tester.run_performance_test('GET', '/api/v1/vm-configs', iterations=50)

            # é©—è­‰æ•ˆèƒ½ç›®æ¨™
            assert stats['avg_response_time'] < performance_tester.performance_target, \
                f"å¹³å‡å›æ‡‰æ™‚é–“ {stats['avg_response_time']:.3f}s è¶…éç›®æ¨™ {performance_tester.performance_target}s"

            assert stats['p95_response_time'] < performance_tester.performance_target * 1.5, \
                f"95th ç™¾åˆ†ä½å›æ‡‰æ™‚é–“ {stats['p95_response_time']:.3f}s è¶…éå®¹å¿ç¯„åœ"

            assert stats['success_rate'] >= 95, \
                f"æˆåŠŸç‡ {stats['success_rate']:.1f}% ä½æ–¼ 95% è¦æ±‚"

    def test_create_vm_config_performance(self, performance_tester, mock_dependencies):
        """æ¸¬è©¦ POST /api/v1/vm-configs æ•ˆèƒ½"""
        config_data = {
            'name': 'æ•ˆèƒ½æ¸¬è©¦å¢é›†',
            'description': 'ç”¨æ–¼æ•ˆèƒ½æ¸¬è©¦çš„å¢é›†',
            'nodes': [
                {
                    'name': 'master-1',
                    'ip': '192.168.1.10',
                    'roles': ['master', 'etcd'],
                    'specs': {'cpu': 2, 'memory': '4Gi', 'disk': '20Gi'}
                }
            ],
            'ssh_user': 'ubuntu',
            'created_by': 'perf_test'
        }

        with patch('src.api.endpoints.vm_configs.create_vm_config') as mock_create:
            mock_create.return_value = {'id': 'perf-test-001', **config_data}

            stats = performance_tester.run_performance_test(
                'POST',
                '/api/v1/vm-configs',
                iterations=30,
                json=config_data
            )

            assert stats['avg_response_time'] < performance_tester.performance_target, \
                f"å»ºç«‹é…ç½®å¹³å‡å›æ‡‰æ™‚é–“ {stats['avg_response_time']:.3f}s è¶…éç›®æ¨™"


class TestQuestionSetAPIPerformance:
    """é¡Œçµ„ API æ•ˆèƒ½æ¸¬è©¦"""

    def test_get_question_sets_performance(self, performance_tester, mock_dependencies):
        """æ¸¬è©¦ GET /api/v1/question-sets æ•ˆèƒ½"""
        mock_question_sets = {
            f'cka/set-{i}': {
                'metadata': {
                    'exam_type': 'CKA',
                    'set_id': f'set-{i}',
                    'name': f'CKA æ¸¬è©¦é›† {i}',
                    'description': f'æ¸¬è©¦ç”¨é¡Œçµ„ {i}',
                    'time_limit_minutes': 120,
                    'passing_score': 70.0
                },
                'questions': [
                    {
                        'id': j,
                        'content': f'æ¸¬è©¦é¡Œç›® {j}',
                        'weight': 25.0,
                        'kubernetes_objects': ['Pod'],
                        'hints': [f'æç¤º {j}']
                    } for j in range(1, 5)
                ]
            } for i in range(5)
        }

        with patch('src.services.question_set_file_manager.QuestionSetFileManager.get_all_question_sets') as mock_get:
            mock_get.return_value = mock_question_sets

            stats = performance_tester.run_performance_test('GET', '/api/v1/question-sets', iterations=50)

            assert stats['avg_response_time'] < performance_tester.performance_target, \
                f"é¡Œçµ„åˆ—è¡¨å›æ‡‰æ™‚é–“ {stats['avg_response_time']:.3f}s è¶…éç›®æ¨™"

    def test_get_single_question_set_performance(self, performance_tester, mock_dependencies):
        """æ¸¬è©¦ GET /api/v1/question-sets/{set_id} æ•ˆèƒ½"""
        mock_question_set = {
            'metadata': {
                'exam_type': 'CKA',
                'set_id': 'perf-test',
                'name': 'CKA æ•ˆèƒ½æ¸¬è©¦é›†',
                'description': 'æ•ˆèƒ½æ¸¬è©¦ç”¨é¡Œçµ„',
                'time_limit_minutes': 120,
                'passing_score': 70.0
            },
            'questions': [
                {
                    'id': i,
                    'content': f'æ•ˆèƒ½æ¸¬è©¦é¡Œç›® {i}',
                    'weight': 25.0,
                    'kubernetes_objects': ['Pod'],
                    'hints': [f'æ•ˆèƒ½æ¸¬è©¦æç¤º {i}']
                } for i in range(1, 21)  # 20 å€‹é¡Œç›®
            ]
        }

        with patch('src.services.question_set_file_manager.QuestionSetFileManager.get_question_set') as mock_get:
            mock_get.return_value = mock_question_set

            stats = performance_tester.run_performance_test(
                'GET',
                '/api/v1/question-sets/cka/perf-test',
                iterations=50
            )

            assert stats['avg_response_time'] < performance_tester.performance_target, \
                f"å–®ä¸€é¡Œçµ„å›æ‡‰æ™‚é–“ {stats['avg_response_time']:.3f}s è¶…éç›®æ¨™"


class TestExamSessionAPIPerformance:
    """è€ƒè©¦æœƒè©± API æ•ˆèƒ½æ¸¬è©¦"""

    def test_create_exam_session_performance(self, performance_tester, mock_dependencies):
        """æ¸¬è©¦ POST /api/v1/exam-sessions æ•ˆèƒ½"""
        session_data = {
            'question_set_id': 'cka/perf-test',
            'vm_cluster_config_id': 'perf-cluster'
        }

        mock_session = {
            'id': 'perf-session-001',
            'status': 'created',
            'created_at': '2025-09-24T10:00:00Z',
            **session_data
        }

        with patch('src.services.exam_session_service.ExamSessionService.create_exam_session') as mock_create:
            mock_create.return_value = mock_session

            stats = performance_tester.run_performance_test(
                'POST',
                '/api/v1/exam-sessions',
                iterations=20,
                json=session_data
            )

            # æœƒè©±å»ºç«‹å¯èƒ½æ¶‰åŠæ›´å¤šé‚è¼¯ï¼Œå®¹è¨±ç¨é•·æ™‚é–“
            extended_target = performance_tester.performance_target * 1.5
            assert stats['avg_response_time'] < extended_target, \
                f"å»ºç«‹æœƒè©±å¹³å‡å›æ‡‰æ™‚é–“ {stats['avg_response_time']:.3f}s è¶…éå»¶é•·ç›®æ¨™ {extended_target:.3f}s"

    def test_get_exam_session_performance(self, performance_tester, mock_dependencies):
        """æ¸¬è©¦ GET /api/v1/exam-sessions/{session_id} æ•ˆèƒ½"""
        mock_session = {
            'id': 'perf-session-001',
            'question_set_id': 'cka/perf-test',
            'vm_cluster_config_id': 'perf-cluster',
            'status': 'in_progress',
            'current_question_index': 5,
            'answers': {str(i): {'solution': f'answer {i}'} for i in range(1, 6)},
            'progress': 25.0,
            'created_at': '2025-09-24T10:00:00Z'
        }

        with patch('src.services.exam_session_service.ExamSessionService.get_exam_session') as mock_get:
            mock_get.return_value = mock_session

            stats = performance_tester.run_performance_test(
                'GET',
                '/api/v1/exam-sessions/perf-session-001',
                iterations=50
            )

            assert stats['avg_response_time'] < performance_tester.performance_target, \
                f"å–å¾—æœƒè©±å›æ‡‰æ™‚é–“ {stats['avg_response_time']:.3f}s è¶…éç›®æ¨™"


class TestConcurrentPerformance:
    """ä¸¦ç™¼æ•ˆèƒ½æ¸¬è©¦"""

    @pytest.mark.asyncio
    async def test_concurrent_api_calls(self, mock_dependencies):
        """æ¸¬è©¦ä¸¦ç™¼ API å‘¼å«æ•ˆèƒ½"""
        async def make_request(client, endpoint):
            start_time = time.time()
            response = client.get(endpoint)
            end_time = time.time()
            return {
                'response_time': end_time - start_time,
                'status_code': response.status_code
            }

        client = TestClient(app)

        with patch('src.api.endpoints.vm_configs.get_vm_configs') as mock_get:
            mock_get.return_value = [{'id': 'test', 'name': 'test'}]

            # æ¨¡æ“¬ 10 å€‹ä¸¦ç™¼è«‹æ±‚
            tasks = []
            for i in range(10):
                task = asyncio.create_task(make_request(client, '/api/v1/vm-configs'))
                tasks.append(task)

            results = await asyncio.gather(*tasks)

            # é©—è­‰ä¸¦ç™¼æ•ˆèƒ½
            response_times = [r['response_time'] for r in results]
            avg_concurrent_time = statistics.mean(response_times)

            # ä¸¦ç™¼è«‹æ±‚çš„å¹³å‡æ™‚é–“æ‡‰è©²åœ¨åˆç†ç¯„åœå…§
            assert avg_concurrent_time < 0.5, \
                f"ä¸¦ç™¼è«‹æ±‚å¹³å‡æ™‚é–“ {avg_concurrent_time:.3f}s éé•·"

            # æ‰€æœ‰è«‹æ±‚æ‡‰è©²æˆåŠŸ
            success_count = sum(1 for r in results if r['status_code'] == 200)
            assert success_count == 10, f"ä¸¦ç™¼è«‹æ±‚æˆåŠŸç‡ {success_count}/10"


class TestDatabasePerformance:
    """è³‡æ–™åº«æ•ˆèƒ½æ¸¬è©¦"""

    def test_database_query_performance(self, performance_tester, mock_dependencies):
        """æ¸¬è©¦è³‡æ–™åº«æŸ¥è©¢æ•ˆèƒ½"""
        # æ¨¡æ“¬è¤‡é›œçš„è³‡æ–™åº«æŸ¥è©¢
        mock_query_result = [
            {
                'id': f'session-{i}',
                'question_set_id': f'cka/set-{i%3}',
                'status': 'completed' if i % 2 == 0 else 'in_progress',
                'created_at': f'2025-09-24T{10 + i%12:02d}:00:00Z',
                'final_score': 75.0 + (i % 20)
            } for i in range(100)
        ]

        with patch('src.api.endpoints.exam_sessions.list_exam_sessions') as mock_list:
            mock_list.return_value = mock_query_result

            stats = performance_tester.run_performance_test(
                'GET',
                '/api/v1/exam-sessions?limit=100',
                iterations=20
            )

            assert stats['avg_response_time'] < performance_tester.performance_target, \
                f"å¤§é‡è³‡æ–™æŸ¥è©¢å›æ‡‰æ™‚é–“ {stats['avg_response_time']:.3f}s è¶…éç›®æ¨™"


def generate_performance_report(test_results: Dict[str, Dict[str, Any]]) -> str:
    """ç”Ÿæˆæ•ˆèƒ½æ¸¬è©¦å ±å‘Š"""
    report = ["# API æ•ˆèƒ½æ¸¬è©¦å ±å‘Š", ""]
    report.append(f"æ¸¬è©¦åŸ·è¡Œæ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"æ•ˆèƒ½ç›®æ¨™: <200ms å¹³å‡å›æ‡‰æ™‚é–“")
    report.append("")

    for test_name, stats in test_results.items():
        report.append(f"## {test_name}")
        report.append(f"- å¹³å‡å›æ‡‰æ™‚é–“: {stats['avg_response_time']:.3f}s")
        report.append(f"- ä¸­ä½æ•¸å›æ‡‰æ™‚é–“: {stats['median_response_time']:.3f}s")
        report.append(f"- 95th ç™¾åˆ†ä½: {stats['p95_response_time']:.3f}s")
        report.append(f"- 99th ç™¾åˆ†ä½: {stats['p99_response_time']:.3f}s")
        report.append(f"- æˆåŠŸç‡: {stats['success_rate']:.1f}%")

        # æ•ˆèƒ½è©•ç´š
        if stats['avg_response_time'] < 0.1:
            grade = "å„ªç§€ ğŸŸ¢"
        elif stats['avg_response_time'] < 0.2:
            grade = "è‰¯å¥½ ğŸŸ¡"
        else:
            grade = "éœ€æ”¹å–„ ğŸ”´"

        report.append(f"- æ•ˆèƒ½è©•ç´š: {grade}")
        report.append("")

    return "\n".join(report)


if __name__ == "__main__":
    # åŸ·è¡Œæ•ˆèƒ½æ¸¬è©¦
    pytest.main([__file__, "-v", "--tb=short"])